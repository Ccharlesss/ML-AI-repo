{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d562cb1-b127-4331-8904-1084fd3e0329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HeartDiseaseorAttack  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0                   0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
      "1                   0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
      "2                   0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
      "3                   0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
      "4                   0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
      "\n",
      "   Diabetes  PhysActivity  Fruits  ...  AnyHealthcare  NoDocbcCost  GenHlth  \\\n",
      "0       0.0           0.0     0.0  ...            1.0          0.0      5.0   \n",
      "1       0.0           1.0     0.0  ...            0.0          1.0      3.0   \n",
      "2       0.0           0.0     1.0  ...            1.0          1.0      5.0   \n",
      "3       0.0           1.0     1.0  ...            1.0          0.0      2.0   \n",
      "4       0.0           1.0     1.0  ...            1.0          0.0      2.0   \n",
      "\n",
      "   MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  Income  \n",
      "0      18.0      15.0       1.0  0.0   9.0        4.0     3.0  \n",
      "1       0.0       0.0       0.0  0.0   7.0        6.0     1.0  \n",
      "2      30.0      30.0       1.0  0.0   9.0        4.0     8.0  \n",
      "3       0.0       0.0       0.0  0.0  11.0        3.0     6.0  \n",
      "4       3.0       0.0       0.0  0.0  11.0        5.0     4.0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "### Exercice 1: Get the DataSet and convert it from CSV to pd.DataFrame:\n",
    "import pandas as pd\n",
    "\n",
    "def convert_csv_to_df() -> pd.DataFrame:\n",
    "    # 1) Specify the path to the CSV file:\n",
    "    file_path = \"/Users/romainkuhne/Documents/pandas_interview_training/myenv/Pandas_interview_prep/heart_disease_health_indicators_BRFSS2015.csv\"\n",
    "    # 2) Load the CSV into a dataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    # 3) Return the converted DataFrame:\n",
    "    return df\n",
    "\n",
    "df = convert_csv_to_df()\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8574ecb9-28a4-4d1f-806e-9f7868f5f885",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Feature  Importance\n",
      "17                Age    0.140224\n",
      "12            GenHlth    0.124162\n",
      "3                 BMI    0.113729\n",
      "14           PhysHlth    0.081048\n",
      "19             Income    0.070460\n",
      "0              HighBP    0.060544\n",
      "13           MentHlth    0.059371\n",
      "18          Education    0.051050\n",
      "15           DiffWalk    0.047713\n",
      "1            HighChol    0.044582\n",
      "16                Sex    0.039859\n",
      "5            Diabetes    0.038524\n",
      "4              Smoker    0.024365\n",
      "7              Fruits    0.022423\n",
      "6        PhysActivity    0.021112\n",
      "8             Veggies    0.021041\n",
      "11        NoDocbcCost    0.016234\n",
      "10      AnyHealthcare    0.009622\n",
      "9   HvyAlcoholConsump    0.008639\n",
      "2           CholCheck    0.005298\n"
     ]
    }
   ],
   "source": [
    "# Exercice 2: Utilise Random Forest to Determine which features is the most important for the occurence of Heart Disease:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 2.1) Define the feature (Y) & label set (X):\n",
    "X = df.drop([\"HeartDiseaseorAttack\", \"Stroke\"], axis=1) # Remove 2 labels = {HeartDiseaseOccurence, Stroke}:\n",
    "Y_hd = df[\"HeartDiseaseorAttack\"] # Label set only includes the occurence of heart disease {0:no occurence, 1:occurence}:\n",
    "\n",
    "\n",
    "# 2.2) Create a helper function that performs Grid Search to fine tune hyper parameters:\n",
    "async def perform_grid_search(featureSet: pd.DataFrame, labelSet: pd.Series):\n",
    "    # 2.2.1) Instantiate the model:\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    # 2.2.2) Define the hypeparameter grid:\n",
    "    param_grid = {\n",
    "        'n_estimators': [50,100,200],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "    # 2.2.3) Define the Stratified K-fold Cross Dalidation:\n",
    "    stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    # 2.2.4) Configure Grid Search with Stratified Cross Validation:\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=stratified_kfold, scoring='f1')\n",
    "    # 2.2.5) Fit the Grid Search to find the best hyperparameters:\n",
    "    # - iterates through all combinations of hyperparameters in param_grid.\n",
    "    # A) Splits the data using the stratified k-fold method (cv=stratified_kfold):\n",
    "    # B) Trains the model (RandomForestClassifier) on the training portion of each fold:\n",
    "    # C) Evaluates the F1 score on the validation portion of each fold:\n",
    "    grid_search.fit(featureSet, labelSet)\n",
    "    # 2.2.5) Get the best model after performing grid search that max F1 score:\n",
    "    best_model = grid_search.best_estimator_\n",
    "    # Return the best model\n",
    "    return best_model\n",
    "\n",
    "\n",
    "\n",
    "# 2.3) Create a function that will use random forest to determine the participation of each features:\n",
    "# Feature importance in Random Forest represents how much each feature contributes to making accurate predictions:\n",
    "# Computed by how much a feature improves the model's split criteria (e.g., Gini impurity or entropy) across all trees in the forest:\n",
    "# The contributions of each feature are summed up for all trees.\n",
    "# These sums are normalized to compute the relative importance of each feature.\n",
    "async def compute_feature_importance_hd(featureSet: pd.DataFrame, labelSet: pd.Series) -> pd.DataFrame:\n",
    "    # 2.3.1) Call helper function to perform Stratified Grid Search W/ Cross Validation:\n",
    "    best_model = await perform_grid_search(featureSet, labelSet)\n",
    "    # 2.3.2) Train the model on the entire feature set\n",
    "    best_model.fit(featureSet, labelSet)\n",
    "    # 2.3.3) Evaluates how each feature contributes to the model’s decision-making process.\n",
    "    # For each split in the Random Forest trees:\n",
    "    # It checks how much splitting on a particular feature reduces the chosen impurity metric (either Gini impurity or entropy\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    # 2.3.4) Create a DataFrame of feature importances:\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': featureSet.columns,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "# Call the function:\n",
    "feature_participation_hd_df = await compute_feature_importance_hd(X, Y_hd)\n",
    "print(feature_participation_hd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43760ae7-d5c6-47ac-9487-641ad7c70c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec65cc6-12d2-4289-99c3-1e02196bc886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "713b9526-3254-4213-9dc6-7e36bb7eb86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price       Date       Close        High         Low        Open    Volume\n",
      "0     2020-01-02   68.123718   68.162078   66.837340   66.837340  28132000\n",
      "1     2020-01-03   67.789421   68.379304   67.036329   67.151713  23728000\n",
      "2     2020-01-06   69.460922   69.575007   67.258334   67.258334  34646000\n",
      "3     2020-01-07   69.417580   69.898350   69.270107   69.646760  30054000\n",
      "4     2020-01-08   69.964615   70.326314   69.293024   69.354799  30560000\n",
      "...          ...         ...         ...         ...         ...       ...\n",
      "1253  2024-12-24  197.570007  197.669998  195.197998  196.169998   6809800\n",
      "1254  2024-12-26  197.100006  198.160004  195.869995  196.740005   7907900\n",
      "1255  2024-12-27  194.039993  196.800003  191.972000  196.470001  14693000\n",
      "1256  2024-12-30  192.690002  193.779999  190.360001  190.865005  12209500\n",
      "1257  2024-12-31  190.440002  193.250000  189.580002  192.445007  14355200\n",
      "\n",
      "[1258 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Exercice 3: Fetch financial data from a stock listed in the S&P500 for a timestamp of 5 years:\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "def fetch_raw_data(ticker=\"GOOG\") -> pd.DataFrame:\n",
    "    # 3.1) Define the timeframe from where to gather data:\n",
    "    endDate = date.today() # format: 2025-01-02:\n",
    "    startDate = (endDate - pd.DateOffset(years=5)).date() # format: 2020-01-02\n",
    "    try: # 3.2) Attempt to fetch data:\n",
    "        df = yf.download(tickers=ticker, start=startDate, end=endDate, progress=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't fetch data for {ticker}: str({e}).\")\n",
    "        return pd.DataFrame\n",
    "\n",
    "    # 3.3) Assess if the DataFrame has a multi index, if true remove it:\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Only use the first level of the multi index:\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    # 3.4) Reset the index:\n",
    "    df.reset_index(inplace=True)\n",
    "    # 3.5) Return the raw DataFrame:\n",
    "    return df\n",
    "\n",
    "raw_data = fetch_raw_data()\n",
    "print(raw_data)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abdb2c-e065-4464-9b42-b911fbf559ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
