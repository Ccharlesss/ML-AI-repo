{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d562cb1-b127-4331-8904-1084fd3e0329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HeartDiseaseorAttack  HighBP  HighChol  CholCheck   BMI  Smoker  Stroke  \\\n",
      "0                   0.0     1.0       1.0        1.0  40.0     1.0     0.0   \n",
      "1                   0.0     0.0       0.0        0.0  25.0     1.0     0.0   \n",
      "2                   0.0     1.0       1.0        1.0  28.0     0.0     0.0   \n",
      "3                   0.0     1.0       0.0        1.0  27.0     0.0     0.0   \n",
      "4                   0.0     1.0       1.0        1.0  24.0     0.0     0.0   \n",
      "\n",
      "   Diabetes  PhysActivity  Fruits  ...  AnyHealthcare  NoDocbcCost  GenHlth  \\\n",
      "0       0.0           0.0     0.0  ...            1.0          0.0      5.0   \n",
      "1       0.0           1.0     0.0  ...            0.0          1.0      3.0   \n",
      "2       0.0           0.0     1.0  ...            1.0          1.0      5.0   \n",
      "3       0.0           1.0     1.0  ...            1.0          0.0      2.0   \n",
      "4       0.0           1.0     1.0  ...            1.0          0.0      2.0   \n",
      "\n",
      "   MentHlth  PhysHlth  DiffWalk  Sex   Age  Education  Income  \n",
      "0      18.0      15.0       1.0  0.0   9.0        4.0     3.0  \n",
      "1       0.0       0.0       0.0  0.0   7.0        6.0     1.0  \n",
      "2      30.0      30.0       1.0  0.0   9.0        4.0     8.0  \n",
      "3       0.0       0.0       0.0  0.0  11.0        3.0     6.0  \n",
      "4       3.0       0.0       0.0  0.0  11.0        5.0     4.0  \n",
      "\n",
      "[5 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "### Exercice 1: Get the DataSet and convert it from CSV to pd.DataFrame:\n",
    "import pandas as pd\n",
    "\n",
    "def convert_csv_to_df() -> pd.DataFrame:\n",
    "    # 1) Specify the path to the CSV file:\n",
    "    file_path = \"/Users/romainkuhne/Documents/pandas_interview_training/myenv/Pandas_interview_prep/heart_disease_health_indicators_BRFSS2015.csv\"\n",
    "    # 2) Load the CSV into a dataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    # 3) Return the converted DataFrame:\n",
    "    return df\n",
    "\n",
    "df = convert_csv_to_df()\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8574ecb9-28a4-4d1f-806e-9f7868f5f885",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Feature  Importance\n",
      "17                Age    0.140224\n",
      "12            GenHlth    0.124162\n",
      "3                 BMI    0.113729\n",
      "14           PhysHlth    0.081048\n",
      "19             Income    0.070460\n",
      "0              HighBP    0.060544\n",
      "13           MentHlth    0.059371\n",
      "18          Education    0.051050\n",
      "15           DiffWalk    0.047713\n",
      "1            HighChol    0.044582\n",
      "16                Sex    0.039859\n",
      "5            Diabetes    0.038524\n",
      "4              Smoker    0.024365\n",
      "7              Fruits    0.022423\n",
      "6        PhysActivity    0.021112\n",
      "8             Veggies    0.021041\n",
      "11        NoDocbcCost    0.016234\n",
      "10      AnyHealthcare    0.009622\n",
      "9   HvyAlcoholConsump    0.008639\n",
      "2           CholCheck    0.005298\n"
     ]
    }
   ],
   "source": [
    "# Exercice 2: Utilise Random Forest to Determine which features is the most important for the occurence of Heart Disease:\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 2.1) Define the feature (Y) & label set (X):\n",
    "X = df.drop([\"HeartDiseaseorAttack\", \"Stroke\"], axis=1) # Remove 2 labels = {HeartDiseaseOccurence, Stroke}:\n",
    "Y_hd = df[\"HeartDiseaseorAttack\"] # Label set only includes the occurence of heart disease {0:no occurence, 1:occurence}:\n",
    "\n",
    "\n",
    "# 2.2) Create a helper function that performs Grid Search to fine tune hyper parameters:\n",
    "async def perform_grid_search(featureSet: pd.DataFrame, labelSet: pd.Series):\n",
    "    # 2.2.1) Instantiate the model:\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    # 2.2.2) Define the hypeparameter grid:\n",
    "    param_grid = {\n",
    "        'n_estimators': [50,100,200],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "    # 2.2.3) Define the Stratified K-fold Cross Dalidation:\n",
    "    stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    # 2.2.4) Configure Grid Search with Stratified Cross Validation:\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=stratified_kfold, scoring='f1')\n",
    "    # 2.2.5) Fit the Grid Search to find the best hyperparameters:\n",
    "    # - iterates through all combinations of hyperparameters in param_grid.\n",
    "    # A) Splits the data using the stratified k-fold method (cv=stratified_kfold):\n",
    "    # B) Trains the model (RandomForestClassifier) on the training portion of each fold:\n",
    "    # C) Evaluates the F1 score on the validation portion of each fold:\n",
    "    grid_search.fit(featureSet, labelSet)\n",
    "    # 2.2.5) Get the best model after performing grid search that max F1 score:\n",
    "    best_model = grid_search.best_estimator_\n",
    "    # Return the best model\n",
    "    return best_model\n",
    "\n",
    "\n",
    "\n",
    "# 2.3) Create a function that will use random forest to determine the participation of each features:\n",
    "# Feature importance in Random Forest represents how much each feature contributes to making accurate predictions:\n",
    "# Computed by how much a feature improves the model's split criteria (e.g., Gini impurity or entropy) across all trees in the forest:\n",
    "# The contributions of each feature are summed up for all trees.\n",
    "# These sums are normalized to compute the relative importance of each feature.\n",
    "async def compute_feature_importance_hd(featureSet: pd.DataFrame, labelSet: pd.Series) -> pd.DataFrame:\n",
    "    # 2.3.1) Call helper function to perform Stratified Grid Search W/ Cross Validation:\n",
    "    best_model = await perform_grid_search(featureSet, labelSet)\n",
    "    # 2.3.2) Train the model on the entire feature set\n",
    "    best_model.fit(featureSet, labelSet)\n",
    "    # 2.3.3) Evaluates how each feature contributes to the model’s decision-making process.\n",
    "    # For each split in the Random Forest trees:\n",
    "    # It checks how much splitting on a particular feature reduces the chosen impurity metric (either Gini impurity or entropy\n",
    "    feature_importances = best_model.feature_importances_\n",
    "    # 2.3.4) Create a DataFrame of feature importances:\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': featureSet.columns,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "# Call the function:\n",
    "feature_participation_hd_df = await compute_feature_importance_hd(X, Y_hd)\n",
    "print(feature_participation_hd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "713b9526-3254-4213-9dc6-7e36bb7eb86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price       Date       Close        High         Low        Open    Volume\n",
      "0     2020-01-07   69.417580   69.898350   69.270107   69.646760  30054000\n",
      "1     2020-01-08   69.964615   70.326314   69.293024   69.354799  30560000\n",
      "2     2020-01-09   70.737328   71.110985   70.261035   70.774198  30018000\n",
      "3     2020-01-10   71.230568   71.489586   70.663605   71.122451  36414000\n",
      "4     2020-01-13   71.703865   71.768133   71.045730   71.549421  33046000\n",
      "...          ...         ...         ...         ...         ...       ...\n",
      "1253  2024-12-30  192.690002  193.779999  190.360001  190.865005  12209500\n",
      "1254  2024-12-31  190.440002  193.250000  189.580002  192.445007  14355200\n",
      "1255  2025-01-02  190.630005  193.199997  188.710007  191.485001  17545200\n",
      "1256  2025-01-03  193.130005  194.500000  191.350006  192.725006  12875000\n",
      "1257  2025-01-06  197.960007  199.559998  195.059998  195.149994  19473200\n",
      "\n",
      "[1258 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Exercice 3: Fetch financial data from a stock listed in the S&P500 for a timestamp of 5 years:\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "def fetch_raw_data(ticker=\"GOOG\") -> pd.DataFrame:\n",
    "    # 3.1) Define the timeframe from where to gather data:\n",
    "    endDate = date.today() # format: 2025-01-02:\n",
    "    startDate = (endDate - pd.DateOffset(years=5)).date() # format: 2020-01-02\n",
    "    try: # 3.2) Attempt to fetch data:\n",
    "        df = yf.download(tickers=ticker, start=startDate, end=endDate, progress=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't fetch data for {ticker}: str({e}).\")\n",
    "        return pd.DataFrame\n",
    "\n",
    "    # 3.3) Assess if the DataFrame has a multi index, if true remove it:\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        # Only use the first level of the multi index:\n",
    "        df.columns = df.columns.get_level_values(0)\n",
    "    # 3.4) Reset the index:\n",
    "    df.reset_index(inplace=True)\n",
    "    # 3.5) Return the raw DataFrame:\n",
    "    return df\n",
    "\n",
    "raw_data = fetch_raw_data()\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50abdb2c-e065-4464-9b42-b911fbf559ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price       Date       Close        High         Low        Open    Volume  \\\n",
      "33    2020-02-25   69.173950   71.649552   68.872535   71.393476  49566000   \n",
      "34    2020-02-26   69.409607   70.531583   68.703146   69.557080  44048000   \n",
      "35    2020-02-27   65.668549   68.339646   65.622708   67.859171  59566000   \n",
      "36    2020-02-28   66.726746   66.816921   63.322477   63.646314  75782000   \n",
      "37    2020-03-02   69.206833   69.294523   66.103238   67.338546  48630000   \n",
      "...          ...         ...         ...         ...         ...       ...   \n",
      "1253  2024-12-30  192.690002  193.779999  190.360001  190.865005  12209500   \n",
      "1254  2024-12-31  190.440002  193.250000  189.580002  192.445007  14355200   \n",
      "1255  2025-01-02  190.630005  193.199997  188.710007  191.485001  17545200   \n",
      "1256  2025-01-03  193.130005  194.500000  191.350006  192.725006  12875000   \n",
      "1257  2025-01-06  197.960007  199.559998  195.059998  195.149994  19473200   \n",
      "\n",
      "Price       RSI         K%         D%         R%       PROC      MACD  \\\n",
      "33    -0.246131   4.041224  22.691159 -95.958776  -4.050948  0.194169   \n",
      "34    -0.287759   9.261552   7.250754 -90.738448  -3.801191 -0.137285   \n",
      "35    -0.163266   0.428083   4.576953 -99.571917 -10.712424 -0.693839   \n",
      "36    -0.304466  26.169445  11.953027 -73.830555  -9.457622 -1.037564   \n",
      "37    -0.686310  45.234483  23.944004 -54.765517  -7.925477 -1.097198   \n",
      "...         ...        ...        ...        ...        ...       ...   \n",
      "1253  -1.145247  49.579421  65.945007 -50.420579   8.802934  5.326967   \n",
      "1254  -0.896458  25.150453  47.520332 -74.849547   2.096180  4.781254   \n",
      "1255  -0.917625  25.802588  33.510821 -74.197412  -3.090845  4.314370   \n",
      "1256  -1.238980  40.944917  30.632653 -59.055083  -0.258224  4.098842   \n",
      "1257  -1.955354  70.199908  45.649137 -29.800092   3.438187  4.268570   \n",
      "\n",
      "Price  MACD_Signal  MACD_Diff         OBV  \n",
      "33        0.848165  -0.653996   229054000  \n",
      "34        0.651075  -0.788360   273102000  \n",
      "35        0.382092  -1.075931   213536000  \n",
      "36        0.098161  -1.135725   289318000  \n",
      "37       -0.140911  -0.956287   337948000  \n",
      "...            ...        ...         ...  \n",
      "1253      5.356342  -0.029375  2456417400  \n",
      "1254      5.241325  -0.460071  2442062200  \n",
      "1255      5.055934  -0.741563  2459607400  \n",
      "1256      4.864515  -0.765673  2472482400  \n",
      "1257      4.745326  -0.476757  2491955600  \n",
      "\n",
      "[1225 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# Exercice 4: Create a function to process data in order to compute features:\n",
    "# Ensure that data is cleaned (Nan values are removed):\n",
    "from ta.momentum import StochasticOscillator\n",
    "from ta.momentum import ROCIndicator\n",
    "from ta.trend import MACD\n",
    "from ta.volume import OnBalanceVolumeIndicator\n",
    "\n",
    "# Main async function responsible for orchestrating logic computation of the featureset X:\n",
    "async def compute_financial_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Call helper functions to compute financial features:\n",
    "    df = await compute_RSI(df)\n",
    "    if df is None:\n",
    "        print(\"An error has occured in the computation of the RSI.\")\n",
    "        return None\n",
    "    df = await compute_StochasticOscillators(df)\n",
    "    if df is None:\n",
    "        print(\"An error has occured in the computation of stochastic oscillators.\")\n",
    "        return None\n",
    "    df = await compute_proc(df)\n",
    "    if df is None:\n",
    "        print(\"An error has occured in the computation of the PROC.\")\n",
    "        return None\n",
    "    df = await compute_macd(df)\n",
    "    if df is None:\n",
    "        print(\"An error has occured in the computation of MACD.\")\n",
    "        return None\n",
    "    df = await compute_obv(df)\n",
    "    if df is None:\n",
    "        print(\"An error has occured in the computation of OBV.\")\n",
    "        return None\n",
    "    # Remove Nan Values\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "    \n",
    "# Async helper function to compute RSI:\n",
    "async def compute_RSI(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Ensure the dataFrame is not empty:\n",
    "    if df.empty:\n",
    "        print(\"The DataFrame passed as input to compute features is empty.\")\n",
    "        return None\n",
    "    # Compute Price delta ΔP (DeltaP):\n",
    "    df[\"DeltaP\"] = df[\"Close\"].diff()\n",
    "    # Set the nbr of days = 14:\n",
    "    nbr_days = 14\n",
    "    # Create 2 parallel DataFrames:\n",
    "    up_df = df[\"DeltaP\"].copy()\n",
    "    down_df = df[\"DeltaP\"].copy()\n",
    "    # For up days if ΔP<0 => up_df=0. for Down days, if ΔP>0 => down_df=0:\n",
    "    up_df[up_df<0] = 0\n",
    "    down_df[down_df>0] = 0\n",
    "    # Ensure down_df has no negative values:\n",
    "    down_df = down_df.abs()\n",
    "    # compute Exponential Weighted Moving Average (EWMA): Give more weight to more recent prices:\n",
    "    ewma_up = up_df.transform(lambda x: x.ewm(span=nbr_days).mean())\n",
    "    ewma_down = down_df.transform(lambda x: x.ewm(span=nbr_days).mean())\n",
    "    # Compute the Relative Strength:\n",
    "    relative_strength = ewma_up/ewma_down\n",
    "    # Compute the Relative Strenght Index (RSI):\n",
    "    rsi = 100 - (100/1.0+relative_strength)\n",
    "    # Add the computed rsi into the original dataframe that will contain all financial features:\n",
    "    df[\"RSI\"] = rsi\n",
    "    # Clean the original dataframe to remove ΔP (DeltaP):\n",
    "    df.drop(columns=[\"DeltaP\"], axis=1, inplace=True)\n",
    "    # Assess if the RSI column is empty or contains Nan values:\n",
    "    if df[\"RSI\"].isnull().all():\n",
    "        print(\"The RSI feature is either empty or contains Nan values.\")\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "# Async helper function to compute Stochastic Oscillators = {MACD, MACD_Signal, MACD_Diff}:\n",
    "async def compute_StochasticOscillators(df: pd.DataFrame, window=14, smooth_window=3) -> pd.DataFrame:\n",
    "    # 1) Initialize the stochastic Oscillator:\n",
    "    stoch = StochasticOscillator(high=df[\"High\"],low=df[\"Low\"],close=df[\"Close\"],window=window,smooth_window=smooth_window)\n",
    "    # 2) Compute the K%:\n",
    "    df[\"K%\"] = stoch.stoch()\n",
    "    # 3) Compute the D% (Moving avr of K%):\n",
    "    df[\"D%\"] = stoch.stoch_signal()\n",
    "    # 4) Compute the R%:\n",
    "    df[\"R%\"] = -100*(df[\"High\"].rolling(window=window).max()-df[\"Close\"]) / (df[\"High\"].rolling(window=window).max()-df[\"Low\"].rolling(window=window).min())\n",
    "    # 5) Assert whether the features K%, D% & R% are empty:\n",
    "    if df[[\"K%\", \"D%\", \"R%\"]].isnull().all().any():\n",
    "        print(\"One of more stochastic oscillators features are empty.\")\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "# Async helper function to compute the PROC:\n",
    "async def compute_proc(df: pd.DataFrame, window=14) -> pd.DataFrame:\n",
    "    # 1) Initialize the PROC Indicator:\n",
    "    roc = ROCIndicator(close=df[\"Close\"],window=window)\n",
    "    # 2) Compute PROC:\n",
    "    df[\"PROC\"] = roc.roc()\n",
    "    # 3) Assess whether the PROC column isn't empty:\n",
    "    if df[\"PROC\"].isnull().all():\n",
    "        print(\"The PROC column is either empty or only contains Nan values.\")\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "# Async helper function to compute the MACD:\n",
    "async def compute_macd(df: pd.DataFrame, window_slow=26, window_fast=12, window_sign=9) -> pd.DataFrame:\n",
    "    # 1) Initialize the MACT indicator:\n",
    "    macd = MACD(close=df[\"Close\"], window_slow=window_slow, window_fast=window_fast, window_sign=window_sign)\n",
    "    # 2) Compute the MACD:\n",
    "    df[\"MACD\"] = macd.macd()\n",
    "    # 3) Compute MACD Signal:\n",
    "    df[\"MACD_Signal\"] = macd.macd_signal()\n",
    "    # 4) Compute MACD Diff:\n",
    "    df[\"MACD_Diff\"] = macd.macd_diff()\n",
    "    # 5) Assess if one of all columns are empty:\n",
    "    if df[[\"MACD\", \"MACD_Signal\", \"MACD_Diff\"]].isnull().all().any():\n",
    "        print(\"One or more of the MACD indocators is either empty or only contains Nan values.\")\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "# Async helpter function to compute the OBV (On Balance Value):\n",
    "async def compute_obv(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1) Initialize the OBV indicator:\n",
    "    obv = OnBalanceVolumeIndicator(close=df[\"Close\"], volume=df[\"Volume\"])\n",
    "    # 2) Compute OBV:\n",
    "    df[\"OBV\"] = obv.on_balance_volume()\n",
    "    # 3) Assess whether the OBV column is empty or contains NanValues:\n",
    "    if df[\"OBV\"].isnull().all():\n",
    "        print(\"The OBV feature is either empty or contains Nan values\")\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "df_features = await compute_financial_features(raw_data)\n",
    "print(df_features)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516253fa-df29-4f01-b29b-0a339e6d84a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price       Date       RSI         K%         D%         R%       PROC  \\\n",
      "33    2020-02-25 -0.246131   4.041224  22.691159 -95.958776  -4.050948   \n",
      "34    2020-02-26 -0.287759   9.261552   7.250754 -90.738448  -3.801191   \n",
      "35    2020-02-27 -0.163266   0.428083   4.576953 -99.571917 -10.712424   \n",
      "36    2020-02-28 -0.304466  26.169445  11.953027 -73.830555  -9.457622   \n",
      "37    2020-03-02 -0.686310  45.234483  23.944004 -54.765517  -7.925477   \n",
      "...          ...       ...        ...        ...        ...        ...   \n",
      "1253  2024-12-30 -1.145247  49.579421  65.945007 -50.420579   8.802934   \n",
      "1254  2024-12-31 -0.896458  25.150453  47.520332 -74.849547   2.096180   \n",
      "1255  2025-01-02 -0.917625  25.802588  33.510821 -74.197412  -3.090845   \n",
      "1256  2025-01-03 -1.238980  40.944917  30.632653 -59.055083  -0.258224   \n",
      "1257  2025-01-06 -1.955354  70.199908  45.649137 -29.800092   3.438187   \n",
      "\n",
      "Price      MACD  MACD_Signal  MACD_Diff         OBV  Prediction  \n",
      "33     0.194169     0.848165  -0.653996   229054000           0  \n",
      "34    -0.137285     0.651075  -0.788360   273102000           1  \n",
      "35    -0.693839     0.382092  -1.075931   213536000           0  \n",
      "36    -1.037564     0.098161  -1.135725   289318000           1  \n",
      "37    -1.097198    -0.140911  -0.956287   337948000           1  \n",
      "...         ...          ...        ...         ...         ...  \n",
      "1253   5.326967     5.356342  -0.029375  2456417400           0  \n",
      "1254   4.781254     5.241325  -0.460071  2442062200           0  \n",
      "1255   4.314370     5.055934  -0.741563  2459607400           1  \n",
      "1256   4.098842     4.864515  -0.765673  2472482400           1  \n",
      "1257   4.268570     4.745326  -0.476757  2491955600           1  \n",
      "\n",
      "[1225 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/2bb2z7q96qnf5txnltjfttjm0000gn/T/ipykernel_71842/3382475962.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"Prediction\"] = closed_groups\n"
     ]
    }
   ],
   "source": [
    "# Exercice 5: Generate the prediction column:\n",
    "async def generate_prediction_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1) Set Lebel Trend: if close_f < close_i => Y=0(BULLISH) ELSE Y=1(BULLISH):\n",
    "    closed_groups = df[\"Close\"].transform(lambda x:x.shift(1)<x)\n",
    "    # 2) Convert Boolean values to binary: {1/0}:\n",
    "    closed_groups = closed_groups * 1\n",
    "    # 3) create a New column called prediction Y:\n",
    "    df[\"Prediction\"] = closed_groups\n",
    "    # 4) Clean data and remove unwanted features:\n",
    "    df = df.drop([\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"], axis=1)\n",
    "    # 5) Assess whether the prediction column only contains either 0 or 1 values:\n",
    "    if not df[\"Prediction\"].isin([0,1]).all():\n",
    "        print(\"Error the prediction column doesn't contain binary values.\")\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "prediction_df = await generate_prediction_column(df_features)\n",
    "print(prediction_df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b21bccb-5374-4064-b96d-d4cede7518db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Accuracy: 0.7027\n",
      "Fold 2 - Accuracy: 0.7297\n",
      "Fold 3 - Accuracy: 0.8198\n",
      "Fold 4 - Accuracy: 0.8288\n",
      "Fold 5 - Accuracy: 0.8559\n",
      "Fold 6 - Accuracy: 0.8018\n",
      "Fold 7 - Accuracy: 0.8288\n",
      "Fold 8 - Accuracy: 0.9009\n",
      "Fold 9 - Accuracy: 0.8378\n",
      "Fold 10 - Accuracy: 0.8378\n",
      "Average Accuracy across folds: 0.8144\n",
      "Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Final OOB Score: 0.8090\n",
      "Tomorrow's closing price trend prediction: Higher\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Exercice 6: Time serie cross validation & GRID SEARCH => Predict next closing price:\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error, confusion_matrix\n",
    "\n",
    "async def predict_next_closing_price_trend(df: pd.DataFrame) -> int:\n",
    "    # 1) Divide the Dataset D into features=X and labels=Y:\n",
    "    X = df[[\"RSI\", \"K%\", \"D%\", \"R%\", \"PROC\", \"MACD\", \"MACD_Signal\", \"MACD_Diff\", \"OBV\"]]\n",
    "    y = df[\"Prediction\"]\n",
    "    # 2) Perform Time Serie Cross Validation to evaluate the initial model:\n",
    "    # Purpose: Evaluate how the model performs on unseen data:\n",
    "    tscv = TimeSeriesSplit(n_splits=10) # set to 10 splits:\n",
    "    accuracy_scores = [] # Store accuracy of each fold:\n",
    "    # 2.1) For each fold split into training X_train, y_train, Y_test and y_test:\n",
    "    # tscv.split(X) generates the training and testing indices for each fold:\n",
    "    # enumerate(...): Adds an index (fold_idx) to track which fold is being processed:\n",
    "    for fold_idx, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # 2.2) Train a Random Forest model:\n",
    "        forest = RandomForestClassifier(\n",
    "            n_estimators=100, # Nbr of decision trees in the forest:\n",
    "            max_depth=10, # Sets the max depth of each trees = max level of splits:\n",
    "            max_features='sqrt', # How many features are considered for splitting at each node:\n",
    "            random_state=42, # Ensure reproductibility:\n",
    "            criterion='gini', # measures how often a randomly chosen element from the dataset would be incorrectly classified based on the split:\n",
    "            oob_score=True # Each tree is tested on data points left in the bag:\n",
    "        )\n",
    "\n",
    "        # 2.3) Train the model by injecting the training feature and label:\n",
    "        forest.fit(X_train, y_train)\n",
    "        # 2.4) Evaluate the model on test set:\n",
    "        y_pred = forest.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        print(f\"Fold {fold_idx + 1} - Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # 3) Compute the average accuracy across folds:\n",
    "    avg_accuracy = sum(accuracy_scores)/len(accuracy_scores)\n",
    "    print(f\"Average Accuracy across folds: {avg_accuracy:.4f}\")\n",
    "\n",
    "    # 4) Initialize the Grid Search:\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"max_depth\": [None, 10, 20],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"criterion\": [\"gini\", \"entropy\"]\n",
    "    }\n",
    "    \n",
    "    forest = RandomForestClassifier(random_state=42, oob_score=True)\n",
    "    # 5) Instantiate grid search:\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=forest, # model:\n",
    "        param_grid=param_grid, # grid search parameters to test:\n",
    "        cv=TimeSeriesSplit(n_splits=5),\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # 6) Perform grid search on the entire dataset:\n",
    "    grid_search.fit(X,y)\n",
    "    # 7) Get the fine tunned model:\n",
    "    best_forest = grid_search.best_estimator_\n",
    "    print(f'Best Parameters: {grid_search.best_params_}')\n",
    "    # 8) Train the final model on the entire dataset with best parameters\n",
    "    best_forest.fit(X, y)\n",
    "    # If OOB score is available, print it\n",
    "    if hasattr(best_forest, 'oob_score_'):\n",
    "        print(f'Final OOB Score: {best_forest.oob_score_:.4f}')\n",
    "    # 9) Predict tomorrow's trend using the final model\n",
    "    latest_data = X.iloc[-1:]  # Latest available data (last row of X)\n",
    "    next_day_prediction = best_forest.predict(latest_data)\n",
    "\n",
    "    print(f\"Tomorrow's closing price trend prediction: {'Higher' if int(next_day_prediction[0]) == 1 else 'Lower'}\")\n",
    "    return int(next_day_prediction[0])\n",
    "    \n",
    "    \n",
    "prediction = await predict_next_closing_price_trend(prediction_df)\n",
    "print(prediction)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c956b6-4742-41f4-a110-7e65f0009196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
